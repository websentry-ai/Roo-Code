import { Anthropic } from "@anthropic-ai/sdk"
import OpenAI from "openai"
import {
	convertToAiSdkMessages,
	convertToolsForAiSdk,
	processAiSdkStreamPart,
	consumeAiSdkStream,
	mapToolChoice,
	extractAiSdkErrorMessage,
	extractMessageFromResponseBody,
	handleAiSdkError,
	flattenAiSdkMessagesToStringContent,
} from "../ai-sdk"

vitest.mock("ai", () => ({
	tool: vitest.fn((t) => t),
	jsonSchema: vitest.fn((s) => s),
}))

describe("AI SDK conversion utilities", () => {
	describe("convertToAiSdkMessages", () => {
		it("converts simple string messages", () => {
			const messages: Anthropic.Messages.MessageParam[] = [
				{ role: "user", content: "Hello" },
				{ role: "assistant", content: "Hi there" },
			]

			const result = convertToAiSdkMessages(messages)

			expect(result).toHaveLength(2)
			expect(result[0]).toEqual({ role: "user", content: "Hello" })
			expect(result[1]).toEqual({ role: "assistant", content: "Hi there" })
		})

		it("converts user messages with text content blocks", () => {
			const messages: Anthropic.Messages.MessageParam[] = [
				{
					role: "user",
					content: [{ type: "text", text: "Hello world" }],
				},
			]

			const result = convertToAiSdkMessages(messages)

			expect(result).toHaveLength(1)
			expect(result[0]).toEqual({
				role: "user",
				content: [{ type: "text", text: "Hello world" }],
			})
		})

		it("converts user messages with image content", () => {
			const messages: Anthropic.Messages.MessageParam[] = [
				{
					role: "user",
					content: [
						{ type: "text", text: "What is in this image?" },
						{
							type: "image",
							source: {
								type: "base64",
								media_type: "image/png",
								data: "base64encodeddata",
							},
						},
					],
				},
			]

			const result = convertToAiSdkMessages(messages)

			expect(result).toHaveLength(1)
			expect(result[0]).toEqual({
				role: "user",
				content: [
					{ type: "text", text: "What is in this image?" },
					{
						type: "image",
						image: "data:image/png;base64,base64encodeddata",
						mimeType: "image/png",
					},
				],
			})
		})

		it("converts user messages with URL image content", () => {
			const messages: Anthropic.Messages.MessageParam[] = [
				{
					role: "user",
					content: [
						{ type: "text", text: "What is in this image?" },
						{
							type: "image",
							source: {
								type: "url",
								url: "https://example.com/image.png",
							},
						} as any,
					],
				},
			]

			const result = convertToAiSdkMessages(messages)

			expect(result).toHaveLength(1)
			expect(result[0]).toEqual({
				role: "user",
				content: [
					{ type: "text", text: "What is in this image?" },
					{
						type: "image",
						image: "https://example.com/image.png",
					},
				],
			})
		})

		it("converts tool results into separate tool role messages with resolved tool names", () => {
			const messages: Anthropic.Messages.MessageParam[] = [
				{
					role: "assistant",
					content: [
						{
							type: "tool_use",
							id: "call_123",
							name: "read_file",
							input: { path: "test.ts" },
						},
					],
				},
				{
					role: "user",
					content: [
						{
							type: "tool_result",
							tool_use_id: "call_123",
							content: "Tool result content",
						},
					],
				},
			]

			const result = convertToAiSdkMessages(messages)

			expect(result).toHaveLength(2)
			expect(result[0]).toEqual({
				role: "assistant",
				content: [
					{
						type: "tool-call",
						toolCallId: "call_123",
						toolName: "read_file",
						input: { path: "test.ts" },
					},
				],
			})
			// Tool results now go to role: "tool" messages per AI SDK v6 schema
			expect(result[1]).toEqual({
				role: "tool",
				content: [
					{
						type: "tool-result",
						toolCallId: "call_123",
						toolName: "read_file",
						output: { type: "text", value: "Tool result content" },
					},
				],
			})
		})

		it("uses unknown_tool for tool results without matching tool call", () => {
			const messages: Anthropic.Messages.MessageParam[] = [
				{
					role: "user",
					content: [
						{
							type: "tool_result",
							tool_use_id: "call_orphan",
							content: "Orphan result",
						},
					],
				},
			]

			const result = convertToAiSdkMessages(messages)

			expect(result).toHaveLength(1)
			// Tool results go to role: "tool" messages
			expect(result[0]).toEqual({
				role: "tool",
				content: [
					{
						type: "tool-result",
						toolCallId: "call_orphan",
						toolName: "unknown_tool",
						output: { type: "text", value: "Orphan result" },
					},
				],
			})
		})

		it("separates tool results and text content into different messages", () => {
			const messages: Anthropic.Messages.MessageParam[] = [
				{
					role: "assistant",
					content: [
						{
							type: "tool_use",
							id: "call_123",
							name: "read_file",
							input: { path: "test.ts" },
						},
					],
				},
				{
					role: "user",
					content: [
						{
							type: "tool_result",
							tool_use_id: "call_123",
							content: "File contents here",
						},
						{
							type: "text",
							text: "Please analyze this file",
						},
					],
				},
			]

			const result = convertToAiSdkMessages(messages)

			expect(result).toHaveLength(3)
			expect(result[0]).toEqual({
				role: "assistant",
				content: [
					{
						type: "tool-call",
						toolCallId: "call_123",
						toolName: "read_file",
						input: { path: "test.ts" },
					},
				],
			})
			// Tool results go first in a "tool" message
			expect(result[1]).toEqual({
				role: "tool",
				content: [
					{
						type: "tool-result",
						toolCallId: "call_123",
						toolName: "read_file",
						output: { type: "text", value: "File contents here" },
					},
				],
			})
			// Text content goes in a separate "user" message
			expect(result[2]).toEqual({
				role: "user",
				content: [{ type: "text", text: "Please analyze this file" }],
			})
		})

		it("converts assistant messages with tool use", () => {
			const messages: Anthropic.Messages.MessageParam[] = [
				{
					role: "assistant",
					content: [
						{ type: "text", text: "Let me read that file" },
						{
							type: "tool_use",
							id: "call_456",
							name: "read_file",
							input: { path: "test.ts" },
						},
					],
				},
			]

			const result = convertToAiSdkMessages(messages)

			expect(result).toHaveLength(1)
			expect(result[0]).toEqual({
				role: "assistant",
				content: [
					{ type: "text", text: "Let me read that file" },
					{
						type: "tool-call",
						toolCallId: "call_456",
						toolName: "read_file",
						input: { path: "test.ts" },
					},
				],
			})
		})

		it("handles empty assistant content", () => {
			const messages: Anthropic.Messages.MessageParam[] = [
				{
					role: "assistant",
					content: [],
				},
			]

			const result = convertToAiSdkMessages(messages)

			expect(result).toHaveLength(1)
			expect(result[0]).toEqual({
				role: "assistant",
				content: [{ type: "text", text: "" }],
			})
		})

		it("converts assistant reasoning blocks", () => {
			const messages: Anthropic.Messages.MessageParam[] = [
				{
					role: "assistant",
					content: [
						{ type: "reasoning" as any, text: "Thinking..." },
						{ type: "text", text: "Answer" },
					],
				},
			]

			const result = convertToAiSdkMessages(messages)

			expect(result).toHaveLength(1)
			expect(result[0]).toEqual({
				role: "assistant",
				content: [
					{ type: "reasoning", text: "Thinking..." },
					{ type: "text", text: "Answer" },
				],
			})
		})

		it("converts assistant thinking blocks to reasoning", () => {
			const messages: Anthropic.Messages.MessageParam[] = [
				{
					role: "assistant",
					content: [
						{ type: "thinking" as any, thinking: "Deep thought", signature: "sig" },
						{ type: "text", text: "OK" },
					],
				},
			]

			const result = convertToAiSdkMessages(messages)

			expect(result).toHaveLength(1)
			expect(result[0]).toEqual({
				role: "assistant",
				content: [
					{
						type: "reasoning",
						text: "Deep thought",
						providerOptions: {
							bedrock: { signature: "sig" },
							anthropic: { signature: "sig" },
						},
					},
					{ type: "text", text: "OK" },
				],
			})
		})

		it("converts assistant message-level reasoning_content to reasoning part", () => {
			const messages: Anthropic.Messages.MessageParam[] = [
				{
					role: "assistant",
					content: [{ type: "text", text: "Answer" }],
					reasoning_content: "Thinking...",
				} as any,
			]

			const result = convertToAiSdkMessages(messages)

			expect(result).toHaveLength(1)
			expect(result[0]).toEqual({
				role: "assistant",
				content: [
					{ type: "reasoning", text: "Thinking..." },
					{ type: "text", text: "Answer" },
				],
			})
		})

		it("prefers message-level reasoning_content over reasoning blocks", () => {
			const messages: Anthropic.Messages.MessageParam[] = [
				{
					role: "assistant",
					content: [
						{ type: "reasoning" as any, text: "BLOCK" },
						{ type: "text", text: "Answer" },
					],
					reasoning_content: "MSG",
				} as any,
			]

			const result = convertToAiSdkMessages(messages)

			expect(result).toHaveLength(1)
			expect(result[0]).toEqual({
				role: "assistant",
				content: [
					{ type: "reasoning", text: "MSG" },
					{ type: "text", text: "Answer" },
				],
			})
		})

		it("attaches thoughtSignature to first tool-call part for Gemini 3 round-tripping", () => {
			const messages: Anthropic.Messages.MessageParam[] = [
				{
					role: "assistant",
					content: [
						{ type: "text", text: "Let me check that." },
						{
							type: "tool_use",
							id: "tool-1",
							name: "read_file",
							input: { path: "test.txt" },
						},
						{ type: "thoughtSignature", thoughtSignature: "encrypted-sig-abc" } as any,
					],
				},
			]

			const result = convertToAiSdkMessages(messages)

			expect(result).toHaveLength(1)
			const assistantMsg = result[0]
			expect(assistantMsg.role).toBe("assistant")

			const content = assistantMsg.content as any[]
			expect(content).toHaveLength(2) // text + tool-call (thoughtSignature block is consumed, not passed through)

			const toolCallPart = content.find((p: any) => p.type === "tool-call")
			expect(toolCallPart).toBeDefined()
			expect(toolCallPart.providerOptions).toEqual({
				google: { thoughtSignature: "encrypted-sig-abc" },
				vertex: { thoughtSignature: "encrypted-sig-abc" },
			})
		})

		it("attaches thoughtSignature only to the first tool-call in parallel calls", () => {
			const messages: Anthropic.Messages.MessageParam[] = [
				{
					role: "assistant",
					content: [
						{
							type: "tool_use",
							id: "tool-1",
							name: "get_weather",
							input: { city: "Paris" },
						},
						{
							type: "tool_use",
							id: "tool-2",
							name: "get_weather",
							input: { city: "London" },
						},
						{ type: "thoughtSignature", thoughtSignature: "sig-parallel" } as any,
					],
				},
			]

			const result = convertToAiSdkMessages(messages)
			const content = (result[0] as any).content as any[]

			const toolCalls = content.filter((p: any) => p.type === "tool-call")
			expect(toolCalls).toHaveLength(2)

			// Only the first tool call should have the signature
			expect(toolCalls[0].providerOptions).toEqual({
				google: { thoughtSignature: "sig-parallel" },
				vertex: { thoughtSignature: "sig-parallel" },
			})
			// Second tool call should NOT have the signature
			expect(toolCalls[1].providerOptions).toBeUndefined()
		})

		it("does not attach providerOptions when no thoughtSignature block is present", () => {
			const messages: Anthropic.Messages.MessageParam[] = [
				{
					role: "assistant",
					content: [
						{ type: "text", text: "Using tool" },
						{
							type: "tool_use",
							id: "tool-1",
							name: "read_file",
							input: { path: "test.txt" },
						},
					],
				},
			]

			const result = convertToAiSdkMessages(messages)
			const content = (result[0] as any).content as any[]
			const toolCallPart = content.find((p: any) => p.type === "tool-call")

			expect(toolCallPart).toBeDefined()
			expect(toolCallPart.providerOptions).toBeUndefined()
		})

		it("attaches valid reasoning_details as providerOptions.openrouter, filtering invalid entries", () => {
			const validEncrypted = {
				type: "reasoning.encrypted",
				data: "encrypted_blob_data",
				id: "tool_call_123",
				format: "google-gemini-v1",
				index: 0,
			}
			const invalidEncrypted = {
				// type is "reasoning.encrypted" but has text instead of data —
				// this is a plaintext summary mislabeled as encrypted by Gemini/OpenRouter.
				// The provider's ReasoningDetailEncryptedSchema requires `data: string`,
				// so including this causes the entire Zod safeParse to fail.
				type: "reasoning.encrypted",
				text: "Plaintext reasoning summary",
				id: "tool_call_123",
				format: "google-gemini-v1",
				index: 0,
			}
			const textWithSignature = {
				type: "reasoning.text",
				text: "Some reasoning content",
				signature: "stale-signature-from-previous-model",
			}

			const messages: Anthropic.Messages.MessageParam[] = [
				{
					role: "assistant",
					content: [
						{ type: "text", text: "Using a tool" },
						{
							type: "tool_use",
							id: "tool_call_123",
							name: "attempt_completion",
							input: { result: "done" },
						},
					],
					reasoning_details: [validEncrypted, invalidEncrypted, textWithSignature],
				} as any,
			]

			const result = convertToAiSdkMessages(messages)

			expect(result).toHaveLength(1)
			const assistantMsg = result[0] as any
			expect(assistantMsg.role).toBe("assistant")
			expect(assistantMsg.providerOptions).toBeDefined()
			expect(assistantMsg.providerOptions.openrouter).toBeDefined()
			const details = assistantMsg.providerOptions.openrouter.reasoning_details
			// Only the valid entries should survive filtering (invalidEncrypted dropped)
			expect(details).toHaveLength(2)
			expect(details[0]).toEqual(validEncrypted)
			// Signatures should be preserved as-is for same-model Anthropic conversations via OpenRouter
			expect(details[1]).toEqual(textWithSignature)
		})

		it("does not attach providerOptions when no reasoning_details are present", () => {
			const messages: Anthropic.Messages.MessageParam[] = [
				{
					role: "assistant",
					content: [{ type: "text", text: "Just text" }],
				},
			]

			const result = convertToAiSdkMessages(messages)

			expect(result).toHaveLength(1)
			const assistantMsg = result[0] as any
			expect(assistantMsg.providerOptions).toBeUndefined()
		})

		it("does not attach providerOptions when reasoning_details is an empty array", () => {
			const messages: Anthropic.Messages.MessageParam[] = [
				{
					role: "assistant",
					content: [{ type: "text", text: "Just text" }],
					reasoning_details: [],
				} as any,
			]

			const result = convertToAiSdkMessages(messages)

			expect(result).toHaveLength(1)
			const assistantMsg = result[0] as any
			expect(assistantMsg.providerOptions).toBeUndefined()
		})

		it("preserves both reasoning_details and thoughtSignature providerOptions", () => {
			const reasoningDetails = [
				{
					type: "reasoning.encrypted",
					data: "encrypted_data",
					id: "tool_call_abc",
					format: "google-gemini-v1",
					index: 0,
				},
			]

			const messages: Anthropic.Messages.MessageParam[] = [
				{
					role: "assistant",
					content: [
						{ type: "thoughtSignature", thoughtSignature: "sig-xyz" } as any,
						{ type: "text", text: "Using tool" },
						{
							type: "tool_use",
							id: "tool_call_abc",
							name: "read_file",
							input: { path: "test.ts" },
						},
					],
					reasoning_details: reasoningDetails,
				} as any,
			]

			const result = convertToAiSdkMessages(messages)

			expect(result).toHaveLength(1)
			const assistantMsg = result[0] as any
			// Message-level providerOptions carries reasoning_details
			expect(assistantMsg.providerOptions.openrouter.reasoning_details).toEqual(reasoningDetails)
			// Part-level providerOptions carries thoughtSignature on the first tool-call
			const toolCallPart = assistantMsg.content.find((p: any) => p.type === "tool-call")
			expect(toolCallPart.providerOptions.google.thoughtSignature).toBe("sig-xyz")
		})
	})

	describe("convertToolsForAiSdk", () => {
		it("returns undefined for empty tools", () => {
			expect(convertToolsForAiSdk(undefined)).toBeUndefined()
			expect(convertToolsForAiSdk([])).toBeUndefined()
		})

		it("converts function tools to AI SDK format", () => {
			const tools: OpenAI.Chat.ChatCompletionTool[] = [
				{
					type: "function",
					function: {
						name: "read_file",
						description: "Read a file from disk",
						parameters: {
							type: "object",
							properties: {
								path: { type: "string", description: "File path" },
							},
							required: ["path"],
						},
					},
				},
			]

			const result = convertToolsForAiSdk(tools)

			expect(result).toBeDefined()
			expect(result!.read_file).toBeDefined()
			expect(result!.read_file.description).toBe("Read a file from disk")
		})

		it("converts multiple tools", () => {
			const tools: OpenAI.Chat.ChatCompletionTool[] = [
				{
					type: "function",
					function: {
						name: "read_file",
						description: "Read a file",
						parameters: {},
					},
				},
				{
					type: "function",
					function: {
						name: "write_file",
						description: "Write a file",
						parameters: {},
					},
				},
			]

			const result = convertToolsForAiSdk(tools)

			expect(result).toBeDefined()
			expect(Object.keys(result!)).toHaveLength(2)
			expect(result!.read_file).toBeDefined()
			expect(result!.write_file).toBeDefined()
		})
	})

	describe("processAiSdkStreamPart", () => {
		it("processes text-delta chunks", () => {
			const part = { type: "text-delta" as const, id: "1", text: "Hello" }
			const chunks = [...processAiSdkStreamPart(part)]

			expect(chunks).toHaveLength(1)
			expect(chunks[0]).toEqual({ type: "text", text: "Hello" })
		})

		it("processes text chunks (fullStream format)", () => {
			const part = { type: "text" as const, text: "Hello from fullStream" }
			const chunks = [...processAiSdkStreamPart(part as any)]

			expect(chunks).toHaveLength(1)
			expect(chunks[0]).toEqual({ type: "text", text: "Hello from fullStream" })
		})

		it("processes reasoning-delta chunks", () => {
			const part = { type: "reasoning-delta" as const, id: "1", text: "thinking..." }
			const chunks = [...processAiSdkStreamPart(part)]

			expect(chunks).toHaveLength(1)
			expect(chunks[0]).toEqual({ type: "reasoning", text: "thinking..." })
		})

		it("processes reasoning chunks (fullStream format)", () => {
			const part = { type: "reasoning" as const, text: "reasoning from fullStream" }
			const chunks = [...processAiSdkStreamPart(part as any)]

			expect(chunks).toHaveLength(1)
			expect(chunks[0]).toEqual({ type: "reasoning", text: "reasoning from fullStream" })
		})

		it("processes tool-input-start chunks", () => {
			const part = { type: "tool-input-start" as const, id: "call_1", toolName: "read_file" }
			const chunks = [...processAiSdkStreamPart(part)]

			expect(chunks).toHaveLength(1)
			expect(chunks[0]).toEqual({ type: "tool_call_start", id: "call_1", name: "read_file" })
		})

		it("processes tool-input-delta chunks", () => {
			const part = { type: "tool-input-delta" as const, id: "call_1", delta: '{"path":' }
			const chunks = [...processAiSdkStreamPart(part)]

			expect(chunks).toHaveLength(1)
			expect(chunks[0]).toEqual({ type: "tool_call_delta", id: "call_1", delta: '{"path":' })
		})

		it("processes tool-input-end chunks", () => {
			const part = { type: "tool-input-end" as const, id: "call_1" }
			const chunks = [...processAiSdkStreamPart(part)]

			expect(chunks).toHaveLength(1)
			expect(chunks[0]).toEqual({ type: "tool_call_end", id: "call_1" })
		})

		it("ignores tool-call chunks to prevent duplicate tools in UI", () => {
			// tool-call is intentionally ignored because tool-input-start/delta/end already
			// provide complete tool call information. Emitting tool-call would cause duplicate
			// tools in the UI for AI SDK providers (e.g., DeepSeek, Moonshot).
			const part = {
				type: "tool-call" as const,
				toolCallId: "call_1",
				toolName: "read_file",
				input: { path: "test.ts" },
			}
			const chunks = [...processAiSdkStreamPart(part)]

			expect(chunks).toHaveLength(0)
		})

		it("processes source chunks with URL", () => {
			const part = {
				type: "source" as const,
				url: "https://example.com",
				title: "Example Source",
			}
			const chunks = [...processAiSdkStreamPart(part as any)]

			expect(chunks).toHaveLength(1)
			expect(chunks[0]).toEqual({
				type: "grounding",
				sources: [
					{
						title: "Example Source",
						url: "https://example.com",
						snippet: undefined,
					},
				],
			})
		})

		it("processes error chunks", () => {
			const part = { type: "error" as const, error: new Error("Test error") }
			const chunks = [...processAiSdkStreamPart(part)]

			expect(chunks).toHaveLength(1)
			expect(chunks[0]).toEqual({
				type: "error",
				error: "StreamError",
				message: "Test error",
			})
		})

		it("ignores lifecycle events", () => {
			const lifecycleEvents = [
				{ type: "text-start" as const },
				{ type: "text-end" as const },
				{ type: "reasoning-start" as const },
				{ type: "reasoning-end" as const },
				{ type: "start-step" as const },
				{ type: "finish-step" as const },
				{ type: "start" as const },
				{ type: "finish" as const },
				{ type: "abort" as const },
			]

			for (const event of lifecycleEvents) {
				const chunks = [...processAiSdkStreamPart(event as any)]
				expect(chunks).toHaveLength(0)
			}
		})
		it("should filter [REDACTED] from reasoning-delta parts", () => {
			const redactedPart = { type: "reasoning-delta" as const, text: "[REDACTED]" }
			const normalPart = { type: "reasoning-delta" as const, text: "actual reasoning" }

			const redactedResult = [...processAiSdkStreamPart(redactedPart as any)]
			const normalResult = [...processAiSdkStreamPart(normalPart as any)]

			expect(redactedResult).toEqual([])
			expect(normalResult).toEqual([{ type: "reasoning", text: "actual reasoning" }])
		})

		it("should filter [REDACTED] from reasoning (fullStream format) parts", () => {
			const redactedPart = { type: "reasoning" as const, text: "[REDACTED]" }
			const normalPart = { type: "reasoning" as const, text: "actual reasoning" }

			const redactedResult = [...processAiSdkStreamPart(redactedPart as any)]
			const normalResult = [...processAiSdkStreamPart(normalPart as any)]

			expect(redactedResult).toEqual([])
			expect(normalResult).toEqual([{ type: "reasoning", text: "actual reasoning" }])
		})
	})

	describe("mapToolChoice", () => {
		it("should return undefined for null or undefined", () => {
			expect(mapToolChoice(null)).toBeUndefined()
			expect(mapToolChoice(undefined)).toBeUndefined()
		})

		it("should handle string tool choices", () => {
			expect(mapToolChoice("auto")).toBe("auto")
			expect(mapToolChoice("none")).toBe("none")
			expect(mapToolChoice("required")).toBe("required")
		})

		it("should return auto for unknown string values", () => {
			expect(mapToolChoice("unknown")).toBe("auto")
			expect(mapToolChoice("invalid")).toBe("auto")
		})

		it("should handle object tool choice with function name", () => {
			const result = mapToolChoice({
				type: "function",
				function: { name: "my_tool" },
			})

			expect(result).toEqual({ type: "tool", toolName: "my_tool" })
		})

		it("should return undefined for object without function name", () => {
			const result = mapToolChoice({
				type: "function",
				function: {},
			})

			expect(result).toBeUndefined()
		})

		it("should return undefined for object with non-function type", () => {
			const result = mapToolChoice({
				type: "other",
				function: { name: "my_tool" },
			})

			expect(result).toBeUndefined()
		})
	})

	describe("extractAiSdkErrorMessage", () => {
		it("should return 'Unknown error' for null/undefined", () => {
			expect(extractAiSdkErrorMessage(null)).toBe("Unknown error")
			expect(extractAiSdkErrorMessage(undefined)).toBe("Unknown error")
		})

		it("should extract message from AI_RetryError", () => {
			const retryError = {
				name: "AI_RetryError",
				message: "Failed after 3 attempts",
				errors: [new Error("Error 1"), new Error("Error 2"), new Error("Too Many Requests")],
				lastError: { message: "Too Many Requests", status: 429 },
			}

			const result = extractAiSdkErrorMessage(retryError)
			expect(result).toBe("Failed after 3 attempts (429): Too Many Requests")
		})

		it("should handle AI_RetryError without status", () => {
			const retryError = {
				name: "AI_RetryError",
				message: "Failed after 2 attempts",
				errors: [new Error("Error 1"), new Error("Connection failed")],
				lastError: { message: "Connection failed" },
			}

			const result = extractAiSdkErrorMessage(retryError)
			expect(result).toBe("Failed after 2 attempts: Connection failed")
		})

		it("should extract message from AI_APICallError", () => {
			const apiError = {
				name: "AI_APICallError",
				message: "Rate limit exceeded",
				status: 429,
			}

			const result = extractAiSdkErrorMessage(apiError)
			expect(result).toBe("API Error (429): Rate limit exceeded")
		})

		it("should handle AI_APICallError without status", () => {
			const apiError = {
				name: "AI_APICallError",
				message: "Connection timeout",
			}

			const result = extractAiSdkErrorMessage(apiError)
			expect(result).toBe("Connection timeout")
		})

		it("should extract message from standard Error", () => {
			const error = new Error("Something went wrong")
			expect(extractAiSdkErrorMessage(error)).toBe("Something went wrong")
		})

		it("should convert non-Error to string", () => {
			expect(extractAiSdkErrorMessage("string error")).toBe("string error")
			expect(extractAiSdkErrorMessage({ custom: "object" })).toBe("[object Object]")
		})

		it("should extract message from AI_APICallError responseBody with JSON error", () => {
			const apiError = {
				name: "AI_APICallError",
				message: "API call failed",
				responseBody: '{"error":{"message":"Insufficient balance or no resource package.","code":"1113"}}',
				statusCode: 402,
			}

			const result = extractAiSdkErrorMessage(apiError)
			expect(result).toContain("Insufficient balance")
			expect(result).not.toBe("API call failed")
		})

		it("should fall back to message when AI_APICallError responseBody is non-JSON", () => {
			const apiError = {
				name: "AI_APICallError",
				message: "Server error",
				responseBody: "Internal Server Error",
				statusCode: 500,
			}

			const result = extractAiSdkErrorMessage(apiError)
			expect(result).toContain("Server error")
		})

		it("should extract message from AI_RetryError lastError responseBody", () => {
			const retryError = {
				name: "AI_RetryError",
				message: "Failed after retries",
				lastError: {
					name: "AI_APICallError",
					message: "API call failed",
					responseBody: '{"error":{"message":"Rate limit exceeded"}}',
					statusCode: 429,
				},
				errors: [{}],
			}

			const result = extractAiSdkErrorMessage(retryError)
			expect(result).toContain("Rate limit exceeded")
		})

		it("should extract message from NoOutputGeneratedError with APICallError cause", () => {
			const error = {
				name: "AI_NoOutputGeneratedError",
				message: "No output generated",
				cause: {
					name: "AI_APICallError",
					message: "Forbidden",
					responseBody: '{"error":{"message":"Insufficient balance"}}',
					statusCode: 403,
				},
			}

			const result = extractAiSdkErrorMessage(error)
			expect(result).toContain("Insufficient balance")
			expect(result).not.toBe("No output generated")
		})

		it("should return own message from NoOutputGeneratedError without useful cause", () => {
			const error = {
				name: "AI_NoOutputGeneratedError",
				message: "No output generated",
			}

			const result = extractAiSdkErrorMessage(error)
			expect(result).toBe("No output generated")
		})
	})

	describe("handleAiSdkError", () => {
		it("should wrap error with provider name", () => {
			const error = new Error("API Error")
			const result = handleAiSdkError(error, "Fireworks")

			expect(result.message).toBe("Fireworks: API Error")
		})

		it("should preserve status code from AI_RetryError", () => {
			const retryError = {
				name: "AI_RetryError",
				errors: [new Error("Too Many Requests")],
				lastError: { message: "Too Many Requests", status: 429 },
			}

			const result = handleAiSdkError(retryError, "SambaNova")

			expect(result.message).toContain("SambaNova:")
			expect(result.message).toContain("429")
			expect((result as any).status).toBe(429)
		})

		it("should preserve status code from AI_APICallError", () => {
			const apiError = {
				name: "AI_APICallError",
				message: "Unauthorized",
				status: 401,
			}

			const result = handleAiSdkError(apiError, "DeepSeek")

			expect(result.message).toContain("DeepSeek:")
			expect(result.message).toContain("401")
			expect((result as any).status).toBe(401)
		})

		it("should preserve original error as cause", () => {
			const originalError = new Error("Original error")
			const result = handleAiSdkError(originalError, "Mistral")

			expect((result as any).cause).toBe(originalError)
		})
	})

	describe("extractMessageFromResponseBody", () => {
		it("should extract message with code from error object", () => {
			const body = '{"error": {"message": "Insufficient balance", "code": "1113"}}'
			expect(extractMessageFromResponseBody(body)).toBe("[1113] Insufficient balance")
		})

		it("should extract message from error object without code", () => {
			const body = '{"error": {"message": "Rate limit exceeded"}}'
			expect(extractMessageFromResponseBody(body)).toBe("Rate limit exceeded")
		})

		it("should extract message from error string field", () => {
			const body = '{"error": "Something went wrong"}'
			expect(extractMessageFromResponseBody(body)).toBe("Something went wrong")
		})

		it("should extract message from top-level message field", () => {
			const body = '{"message": "Bad request"}'
			expect(extractMessageFromResponseBody(body)).toBe("Bad request")
		})

		it("should return undefined for non-JSON string", () => {
			expect(extractMessageFromResponseBody("Not Found")).toBeUndefined()
		})

		it("should return undefined for empty string", () => {
			expect(extractMessageFromResponseBody("")).toBeUndefined()
		})

		it("should return undefined for JSON without error fields", () => {
			const body = '{"status": "ok"}'
			expect(extractMessageFromResponseBody(body)).toBeUndefined()
		})
	})

	describe("flattenAiSdkMessagesToStringContent", () => {
		it("should return messages unchanged if content is already a string", () => {
			const messages = [
				{ role: "user" as const, content: "Hello" },
				{ role: "assistant" as const, content: "Hi there" },
			]

			const result = flattenAiSdkMessagesToStringContent(messages)

			expect(result).toEqual(messages)
		})

		it("should flatten user messages with only text parts to string", () => {
			const messages = [
				{
					role: "user" as const,
					content: [
						{ type: "text" as const, text: "Hello" },
						{ type: "text" as const, text: "World" },
					],
				},
			]

			const result = flattenAiSdkMessagesToStringContent(messages)

			expect(result).toHaveLength(1)
			expect(result[0].role).toBe("user")
			expect(result[0].content).toBe("Hello\nWorld")
		})

		it("should flatten assistant messages with only text parts to string", () => {
			const messages = [
				{
					role: "assistant" as const,
					content: [{ type: "text" as const, text: "I am an assistant" }],
				},
			]

			const result = flattenAiSdkMessagesToStringContent(messages)

			expect(result).toHaveLength(1)
			expect(result[0].role).toBe("assistant")
			expect(result[0].content).toBe("I am an assistant")
		})

		it("should not flatten user messages with image parts", () => {
			const messages = [
				{
					role: "user" as const,
					content: [
						{ type: "text" as const, text: "Look at this" },
						{ type: "image" as const, image: "data:image/png;base64,abc123" },
					],
				},
			]

			const result = flattenAiSdkMessagesToStringContent(messages)

			expect(result).toEqual(messages)
		})

		it("should not flatten assistant messages with tool calls", () => {
			const messages = [
				{
					role: "assistant" as const,
					content: [
						{ type: "text" as const, text: "Let me use a tool" },
						{
							type: "tool-call" as const,
							toolCallId: "123",
							toolName: "read_file",
							input: { path: "test.txt" },
						},
					],
				},
			]

			const result = flattenAiSdkMessagesToStringContent(messages)

			expect(result).toEqual(messages)
		})

		it("should not flatten tool role messages", () => {
			const messages = [
				{
					role: "tool" as const,
					content: [
						{
							type: "tool-result" as const,
							toolCallId: "123",
							toolName: "test",
							output: { type: "text" as const, value: "result" },
						},
					],
				},
			] as any

			const result = flattenAiSdkMessagesToStringContent(messages)

			expect(result).toEqual(messages)
		})

		it("should respect flattenUserMessages option", () => {
			const messages = [
				{
					role: "user" as const,
					content: [{ type: "text" as const, text: "Hello" }],
				},
			]

			const result = flattenAiSdkMessagesToStringContent(messages, { flattenUserMessages: false })

			expect(result).toEqual(messages)
		})

		it("should respect flattenAssistantMessages option", () => {
			const messages = [
				{
					role: "assistant" as const,
					content: [{ type: "text" as const, text: "Hi" }],
				},
			]

			const result = flattenAiSdkMessagesToStringContent(messages, { flattenAssistantMessages: false })

			expect(result).toEqual(messages)
		})

		it("should handle mixed message types correctly", () => {
			const messages = [
				{ role: "user" as const, content: "Simple string" },
				{
					role: "user" as const,
					content: [{ type: "text" as const, text: "Text parts" }],
				},
				{
					role: "assistant" as const,
					content: [{ type: "text" as const, text: "Assistant text" }],
				},
				{
					role: "assistant" as const,
					content: [
						{ type: "text" as const, text: "With tool" },
						{ type: "tool-call" as const, toolCallId: "456", toolName: "test", input: {} },
					],
				},
			]

			const result = flattenAiSdkMessagesToStringContent(messages)

			expect(result[0].content).toBe("Simple string") // unchanged
			expect(result[1].content).toBe("Text parts") // flattened
			expect(result[2].content).toBe("Assistant text") // flattened
			expect(result[3]).toEqual(messages[3]) // unchanged (has tool call)
		})

		it("should handle empty text parts", () => {
			const messages = [
				{
					role: "user" as const,
					content: [
						{ type: "text" as const, text: "" },
						{ type: "text" as const, text: "Hello" },
					],
				},
			]

			const result = flattenAiSdkMessagesToStringContent(messages)

			expect(result[0].content).toBe("\nHello")
		})

		it("should strip reasoning parts and flatten text for string-only models", () => {
			const messages = [
				{
					role: "assistant" as const,
					content: [
						{ type: "reasoning" as const, text: "I am thinking about this..." },
						{ type: "text" as const, text: "Here is my answer" },
					],
				},
			]

			const result = flattenAiSdkMessagesToStringContent(messages)

			// Reasoning should be stripped, only text should remain
			expect(result[0].content).toBe("Here is my answer")
		})

		it("should handle messages with only reasoning parts", () => {
			const messages = [
				{
					role: "assistant" as const,
					content: [{ type: "reasoning" as const, text: "Only reasoning, no text" }],
				},
			]

			const result = flattenAiSdkMessagesToStringContent(messages)

			// Should flatten to empty string when only reasoning is present
			expect(result[0].content).toBe("")
		})

		it("should not flatten if tool calls are present with reasoning", () => {
			const messages = [
				{
					role: "assistant" as const,
					content: [
						{ type: "reasoning" as const, text: "Thinking..." },
						{ type: "text" as const, text: "Using tool" },
						{ type: "tool-call" as const, toolCallId: "abc", toolName: "test", input: {} },
					],
				},
			]

			const result = flattenAiSdkMessagesToStringContent(messages)

			// Should not flatten because there's a tool call
			expect(result[0]).toEqual(messages[0])
		})
	})
})

describe("consumeAiSdkStream", () => {
	/**
	 * Helper to create an AsyncIterable from an array of stream parts.
	 */
	async function* createAsyncIterable<T>(items: T[]): AsyncGenerator<T> {
		for (const item of items) {
			yield item
		}
	}

	/**
	 * Helper to collect all chunks from an async generator.
	 * Returns { chunks, error } to support both success and error paths.
	 */
	async function collectStream(stream: AsyncGenerator<unknown>): Promise<{ chunks: unknown[]; error: Error | null }> {
		const chunks: unknown[] = []
		let error: Error | null = null
		try {
			for await (const chunk of stream) {
				chunks.push(chunk)
			}
		} catch (e) {
			error = e instanceof Error ? e : new Error(String(e))
		}
		return { chunks, error }
	}

	it("yields stream chunks from fullStream", async () => {
		const result = {
			fullStream: createAsyncIterable([
				{ type: "text-delta" as const, id: "1", text: "hello" },
				{ type: "text" as const, text: " world" },
			]),
			usage: Promise.resolve({ inputTokens: 5, outputTokens: 10 }),
		}

		const { chunks, error } = await collectStream(consumeAiSdkStream(result as any))

		expect(error).toBeNull()
		// Two text chunks + one usage chunk
		expect(chunks).toHaveLength(3)
		expect(chunks[0]).toEqual({ type: "text", text: "hello" })
		expect(chunks[1]).toEqual({ type: "text", text: " world" })
	})

	it("yields default usage chunk when no usageHandler provided", async () => {
		const result = {
			fullStream: createAsyncIterable([{ type: "text-delta" as const, id: "1", text: "hi" }]),
			usage: Promise.resolve({ inputTokens: 10, outputTokens: 20 }),
		}

		const { chunks, error } = await collectStream(consumeAiSdkStream(result as any))

		expect(error).toBeNull()
		const usageChunk = chunks.find((c: any) => c.type === "usage")
		expect(usageChunk).toEqual({
			type: "usage",
			inputTokens: 10,
			outputTokens: 20,
		})
	})

	it("uses usageHandler when provided", async () => {
		const result = {
			fullStream: createAsyncIterable([{ type: "text-delta" as const, id: "1", text: "hi" }]),
			usage: Promise.resolve({ inputTokens: 10, outputTokens: 20 }),
		}

		async function* customUsageHandler() {
			yield {
				type: "usage" as const,
				inputTokens: 42,
				outputTokens: 84,
				cacheWriteTokens: 5,
				cacheReadTokens: 3,
			}
		}

		const { chunks, error } = await collectStream(consumeAiSdkStream(result as any, customUsageHandler))

		expect(error).toBeNull()
		const usageChunk = chunks.find((c: any) => c.type === "usage")
		expect(usageChunk).toEqual({
			type: "usage",
			inputTokens: 42,
			outputTokens: 84,
			cacheWriteTokens: 5,
			cacheReadTokens: 3,
		})
	})

	/**
	 * THE KEY TEST: Verifies that when the stream contains an error chunk (e.g. "Insufficient balance")
	 * and result.usage rejects with a generic error (AI SDK's NoOutputGeneratedError), the thrown
	 * error preserves the specific stream error message rather than the generic one.
	 */
	it("captures stream error and throws it when usage fails", async () => {
		const usageRejection = Promise.reject(new Error("No output generated. Check the stream for errors."))
		// Prevent unhandled rejection warning — the rejection is intentionally caught inside consumeAiSdkStream
		usageRejection.catch(() => {})

		const result = {
			fullStream: createAsyncIterable([
				{ type: "text-delta" as const, id: "1", text: "partial" },
				{
					type: "error" as const,
					error: new Error("Insufficient balance to complete this request"),
				},
			]),
			usage: usageRejection,
		}

		const { chunks, error } = await collectStream(consumeAiSdkStream(result as any))

		// The error chunk IS still yielded during stream iteration
		const errorChunk = chunks.find((c: any) => c.type === "error")
		expect(errorChunk).toEqual({
			type: "error",
			error: "StreamError",
			message: "Insufficient balance to complete this request",
		})

		// The thrown error uses the captured stream error, NOT the generic usage error
		expect(error).not.toBeNull()
		expect(error!.message).toBe("Insufficient balance to complete this request")
		expect(error!.message).not.toContain("No output generated")
	})

	it("re-throws usage error when no stream error captured", async () => {
		const usageRejection = Promise.reject(new Error("Rate limit exceeded"))
		usageRejection.catch(() => {})

		const result = {
			fullStream: createAsyncIterable([{ type: "text-delta" as const, id: "1", text: "hello" }]),
			usage: usageRejection,
		}

		const { chunks, error } = await collectStream(consumeAiSdkStream(result as any))

		// Text chunk should still be yielded
		expect(chunks).toHaveLength(1)
		expect(chunks[0]).toEqual({ type: "text", text: "hello" })

		// The original usage error is re-thrown since no stream error was captured
		expect(error).not.toBeNull()
		expect(error!.message).toBe("Rate limit exceeded")
	})

	it("captures stream error and throws it when usageHandler fails", async () => {
		const result = {
			fullStream: createAsyncIterable([
				{ type: "text-delta" as const, id: "1", text: "partial" },
				{
					type: "error" as const,
					error: new Error("Insufficient balance to complete this request"),
				},
			]),
			usage: Promise.resolve({ inputTokens: 0, outputTokens: 0 }),
		}

		// eslint-disable-next-line require-yield
		async function* failingUsageHandler(): AsyncGenerator<never> {
			throw new Error("No output generated. Check the stream for errors.")
		}

		const { chunks, error } = await collectStream(consumeAiSdkStream(result as any, failingUsageHandler))

		// Error chunk was yielded during streaming
		const errorChunk = chunks.find((c: any) => c.type === "error")
		expect(errorChunk).toEqual({
			type: "error",
			error: "StreamError",
			message: "Insufficient balance to complete this request",
		})

		// The thrown error uses the captured stream error, not the usageHandler error
		expect(error).not.toBeNull()
		expect(error!.message).toBe("Insufficient balance to complete this request")
		expect(error!.message).not.toContain("No output generated")
	})
})
